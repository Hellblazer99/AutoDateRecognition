{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AutoDateTagging.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNntcrq7Diy6+jGyiczDPZy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hellblazer99/AutoDateRecognition/blob/main/AutoDateTagging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEje3IRjeWgI"
      },
      "source": [
        "# Automatic Date Tagging Model #\n",
        "### This notebook implements a set of function which can automatically extract columns containing dates from a dataset and convert them to a machine understandable format for further processing. <br> It was a course project of [NLP by HSE University from Coursera](https://www.coursera.org/learn/language-processing) ###\n",
        "The notebook is divided into two sections:\n",
        "\n",
        "\n",
        "## 1. Select columns containing dates ##\n",
        "> Here we use a simple 1 layer LSTM for binary classification of data points as `date` and `not a date`. 100 data points are sampled from each row and passed to it. If the network classifies 75 % of those as dates, the column is marked as a date\n",
        "\n",
        "## 2. Converting dates into YYYY-MM-DD format ##\n",
        "> Here we use an LSTM model with attention to recognise dates of various formats and convert them into a standard one. It was trained on 10,000 dates of various formats for 50 epochs. It generally achieves 100% accuracy <br>\n",
        "> This model is the same which is used for Neural Machine Translation. This was a course project of mine (AndrewNG's Deep Learning Course on Coursera) which I modified a bit and included here.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVDZmjHbiJgi"
      },
      "source": [
        "## Before starting you need to download a [zip file](https://drive.google.com/file/d/1X950Oh-0NOFyEHMxJUySREe3wgsTsOJA/view?usp=sharing), unzip it and include those in your runtime at `/content/`. It contains the following files: \n",
        "\n",
        "\n",
        "*   `nmt_utils.py`: It is contains several accessory functions for date recognizer model. It also provides several dates in various formats using a faker module\n",
        "*   `dataset.csv`: This dataset was generated using the above file and other random data from nltk. It consists of 6 features and 4000 rows. Out of 6 features, 3 are date features\n",
        "*   `class_model.json`: This is the LSTM model which we'll use to classify columns as **date** or **non-date** features\n",
        "*   `class_model.h5`: These are the weights for the above model which we'll load to save time from retraining it\n",
        "*   `tokenizer.pkl`: This pickle file contains the tokenizer we'll use feature classfication\n",
        "*   `nmt_weights.h5`: This file contains the weights of our Neural Translation Model (LSTM with attention) which we'll use to recognise dates. We'll load these weights to save time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9tNH4GUmu1q"
      },
      "source": [
        "We start by installing the environment and importing all files required"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JDNbjzUrmC_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "7ce3f099-085d-43c8-a2c7-01f56e07d6b9"
      },
      "source": [
        "!pip install tensorflow==1.2.1\n",
        "!pip install keras==2.0.7\n",
        "!pip install faker"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==1.2.1 in /usr/local/lib/python3.6/dist-packages (1.2.1)\n",
            "Requirement already satisfied: backports.weakref==1.0rc1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.2.1) (1.0rc1)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.2.1) (3.12.4)\n",
            "Requirement already satisfied: html5lib==0.9999999 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.2.1) (0.9999999)\n",
            "Requirement already satisfied: bleach==1.5.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.2.1) (1.5.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.2.1) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.2.1) (1.18.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.2.1) (3.2.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.2.1) (0.35.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.2.1) (1.0.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.2.0->tensorflow==1.2.1) (49.6.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorflow==1.2.1) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorflow==1.2.1) (3.1.0)\n",
            "Requirement already satisfied: keras==2.0.7 in /usr/local/lib/python3.6/dist-packages (2.0.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from keras==2.0.7) (1.15.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.0.7) (3.13)\n",
            "Requirement already satisfied: theano in /usr/local/lib/python3.6/dist-packages (from keras==2.0.7) (1.0.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from theano->keras==2.0.7) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from theano->keras==2.0.7) (1.18.5)\n",
            "Requirement already satisfied: faker in /usr/local/lib/python3.6/dist-packages (4.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.6/dist-packages (from faker) (2.8.1)\n",
            "Requirement already satisfied: text-unidecode==1.3 in /usr/local/lib/python3.6/dist-packages (from faker) (1.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.4->faker) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMlqxk5ovOHU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "08d2a41a-78e2-4f1b-e0b1-43dffea1a4b6"
      },
      "source": [
        "from keras.models import load_model, Model, model_from_json\n",
        "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
        "from keras.layers import RepeatVector, Dense, Activation, Lambda\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "import pickle\n",
        "import itertools\n",
        "from datetime import date\n",
        "from keras.preprocessing import sequence\n",
        "from faker import Faker\n",
        "from nmt_utils import *"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:458: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:459: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:460: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:461: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:462: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:465: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMzVkwfUvquh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "56b92f13-1a95-4ecf-d1c3-ba20c1424ad6"
      },
      "source": [
        "# This is our dataset of 6 columns and 4000 rows\n",
        "df = pd.read_csv(\"dataset.csv\", index_col=0)\n",
        "df.tail()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentences</th>\n",
              "      <th>Date_1</th>\n",
              "      <th>Words</th>\n",
              "      <th>Date_2</th>\n",
              "      <th>Numbers</th>\n",
              "      <th>Date_3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3995</th>\n",
              "      <td>It might be that a long interval would elapse ...</td>\n",
              "      <td>30 oct 1986</td>\n",
              "      <td>associating</td>\n",
              "      <td>saturday october 11 2014</td>\n",
              "      <td>-1734</td>\n",
              "      <td>saturday december 26 1981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3996</th>\n",
              "      <td>During that long interval Starbuck would ever ...</td>\n",
              "      <td>thursday october 3 1996</td>\n",
              "      <td>grown</td>\n",
              "      <td>august 3 1993</td>\n",
              "      <td>6657</td>\n",
              "      <td>january 12 1975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3997</th>\n",
              "      <td>Not only that , but the subtle insanity of Aha...</td>\n",
              "      <td>tuesday july 31 1984</td>\n",
              "      <td>practiced</td>\n",
              "      <td>tuesday june 24 2014</td>\n",
              "      <td>-6193</td>\n",
              "      <td>monday may 12 2008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3998</th>\n",
              "      <td>For however eagerly and impetuously the savage...</td>\n",
              "      <td>thursday october 6 2011</td>\n",
              "      <td>valueless</td>\n",
              "      <td>5 mar 1996</td>\n",
              "      <td>-8931</td>\n",
              "      <td>9 mar 1989</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3999</th>\n",
              "      <td>Nor was Ahab unmindful of another thing .</td>\n",
              "      <td>15 march 2005</td>\n",
              "      <td>Whether</td>\n",
              "      <td>september 16 2016</td>\n",
              "      <td>-494</td>\n",
              "      <td>wednesday march 20 2019</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              Sentences  ...                     Date_3\n",
              "3995  It might be that a long interval would elapse ...  ...  saturday december 26 1981\n",
              "3996  During that long interval Starbuck would ever ...  ...            january 12 1975\n",
              "3997  Not only that , but the subtle insanity of Aha...  ...         monday may 12 2008\n",
              "3998  For however eagerly and impetuously the savage...  ...                 9 mar 1989\n",
              "3999          Nor was Ahab unmindful of another thing .  ...    wednesday march 20 2019\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-THJBzq7ntGF"
      },
      "source": [
        "## Selecting columns containing dates ##\n",
        "\n",
        "---\n",
        "The function below samples $25\\%$ data points from each column and passes them through the date classifier model. If more than $70\\%$ of the sample points are classified as **dates**, the column is classified as a **date column**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwAi2cmJH7lM"
      },
      "source": [
        "def select_date_cols(df):\n",
        "  features = df.columns\n",
        "  date_classifier = load_date_classifier()\n",
        "  tokenizer = load_tokenizer()\n",
        "\n",
        "  TEST_SIZE = int(len(df.index)/4)\n",
        "  MAX_SEQ_LEN = 30\n",
        "  THRESH = int(0.7*TEST_SIZE)\n",
        "\n",
        "  dates = []\n",
        "  for col in features:\n",
        "    test_sample = random.sample(list(df[col].astype(str)), TEST_SIZE)\n",
        "    sequences = tokenizer.texts_to_sequences(test_sample)\n",
        "    seq_matrix = sequence.pad_sequences(sequences, maxlen=MAX_SEQ_LEN)\n",
        "    preds = date_classifier.predict_classes(seq_matrix)\n",
        "    preds = preds.tolist()\n",
        "    if preds.count([1]) > THRESH:\n",
        "      dates.append(col)\n",
        "\n",
        "  print(\"\\nDate Columns: \", dates)\n",
        "  return dates"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIeQnX4P3gAt"
      },
      "source": [
        "# This function loads the date classifier model\n",
        "def load_date_classifier():\n",
        "  date_classifierfile = open('class_model.json', 'r')\n",
        "  date_classifier_json = date_classifierfile.read()\n",
        "  date_classifierfile.close()\n",
        "  date_classifier = model_from_json(date_classifier_json)\n",
        "  date_classifier.load_weights('class_model.h5')\n",
        "  print(\"Classifier model loaded\")\n",
        "  return date_classifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9nys4MIPvNE"
      },
      "source": [
        "# This function loads the tokenizer used in the model\n",
        "def load_tokenizer():\n",
        "  with open('tokenizer.pickle', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)\n",
        "    return tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0R-AmAiyaB1C"
      },
      "source": [
        "# This function passes the dataset to select_date_cols and creates a separate dates dataset\n",
        "def classify_date_cols(df):\n",
        "  df.dropna(inplace=True)\n",
        "  print(\"Columns containing dates\")\n",
        "  dates = select_date_cols(df)\n",
        "  date_dic = {x:df[x] for x in dates}\n",
        "  date_df = pd.DataFrame(date_dic)\n",
        "  date_df.dropna(inplace=True)\n",
        "  print(\"DataFrame of dates\")\n",
        "  print(date_df.head())\n",
        "  return [date_df, dates]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7D-NxYRcT4L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "3ff16b9c-f024-4db6-9299-a4f3e4554614"
      },
      "source": [
        "date_dff, dates_l = classify_date_cols(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Columns containing dates\n",
            "Classifier model loaded\n",
            " 992/1000 [============================>.] - ETA: 0s\n",
            "Date Columns:  ['Date_1', 'Date_2', 'Date_3']\n",
            "DataFrame of dates\n",
            "                       Date_1  ...                     Date_3\n",
            "0                 jun 26 2003  ...        sunday june 21 1998\n",
            "1                 18 sep 1994  ...        tuesday june 5 2007\n",
            "2              4 january 1983  ...    monday november 19 1984\n",
            "3  wednesday february 13 1991  ...      saturday july 15 2000\n",
            "4     thursday august 21 2003  ...  wednesday december 3 2014\n",
            "\n",
            "[5 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7ehhDJnrhe4"
      },
      "source": [
        "Here we have extracted the columns containing dates and created a separate dataframe. Run the cell below to see its head"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCWBtt6kck3T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "442bbc57-5ec4-4310-bbd1-51ca3c07fce6"
      },
      "source": [
        "date_dff.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date_1</th>\n",
              "      <th>Date_2</th>\n",
              "      <th>Date_3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>jun 26 2003</td>\n",
              "      <td>sunday august 31 2008</td>\n",
              "      <td>sunday june 21 1998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>18 sep 1994</td>\n",
              "      <td>4 nov 1999</td>\n",
              "      <td>tuesday june 5 2007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4 january 1983</td>\n",
              "      <td>15 november 1988</td>\n",
              "      <td>monday november 19 1984</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>wednesday february 13 1991</td>\n",
              "      <td>thursday march 24 1983</td>\n",
              "      <td>saturday july 15 2000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>thursday august 21 2003</td>\n",
              "      <td>10 august 1985</td>\n",
              "      <td>wednesday december 3 2014</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       Date_1  ...                     Date_3\n",
              "0                 jun 26 2003  ...        sunday june 21 1998\n",
              "1                 18 sep 1994  ...        tuesday june 5 2007\n",
              "2              4 january 1983  ...    monday november 19 1984\n",
              "3  wednesday february 13 1991  ...      saturday july 15 2000\n",
              "4     thursday august 21 2003  ...  wednesday december 3 2014\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0H_PoqTsSHK"
      },
      "source": [
        "## Converting dates into YYYY-MM-DD format ##\n",
        "---\n",
        "Here we create the LSTM with attention model and load weights to use it for further steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4k4ikVDiLvbM"
      },
      "source": [
        "# Declaring globals for model creation\n",
        "m = len(date_dff.index)\n",
        "Tx = 30\n",
        "Ty = 10\n",
        "n_a = 32 \n",
        "n_s = 64 \n",
        "s0 = np.zeros((m, n_s))\n",
        "c0 = np.zeros((m, n_s))\n",
        "human_vocab = {' ': 0, '.': 1, '/': 2, '0': 3, '1': 4, '2': 5, '3': 6, '4': 7, '5': 8, '6': 9, '7': 10, '8': 11, '9': 12, 'a': 13, 'b': 14,'c': 15,'d': 16,'e': 17,'f': 18, 'g': 19, 'h': 20, 'i': 21, 'j': 22, 'k': 23, 'l': 24, 'm': 25, 'n': 26, 'o': 27, 'p': 28, 'q': 29, 'r': 30, 's': 31, 't': 32, 'u': 33, 'v': 34, 'w': 35, 'x': 36, 'y': 37, '<unk>': 38, '<pad>': 39  }\n",
        "machine_vocab = {'-': 0, '0': 1, '1': 2, '2': 3, '3': 4, '4': 5, '5': 6, '6': 7, '7': 8, '8': 9, '9': 10}\n",
        "inv_machine_vocab = {0: '-', 1: '0', 2: '1', 3: '2', 4: '3', 5: '4', 6: '5', 7: '6', 8: '7', 9: '8', 10: '9'}\n",
        "post_activation_LSTM_cell = LSTM(n_s, return_state = True)\n",
        "output_layer = Dense(len(machine_vocab), activation=softmax)\n",
        "repeator = RepeatVector(Tx)\n",
        "concatenator = Concatenate(axis=-1)\n",
        "densor1 = Dense(10, activation = \"tanh\")\n",
        "densor2 = Dense(1, activation = \"relu\")\n",
        "activator = Activation(softmax, name='attention_weights')\n",
        "dotor = Dot(axes = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4weZoDkonRN"
      },
      "source": [
        "def one_step_attention(a, s_prev):\n",
        "    \"\"\" \n",
        "    Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights\n",
        "    \"alphas\" and the hidden states \"a\" of the Bi-LSTM.\n",
        "    \"\"\"\n",
        "\n",
        "    # Repeator is used to repeat s_prev to be of shape (m, Tx, n_s) so that we can concatenate it with all hidden states \"a\"\n",
        "    s_prev = repeator(s_prev)\n",
        "    # We use concatenator to concatenate a and s_prev on the last axis\n",
        "    concat = concatenator([a, s_prev])\n",
        "    # We use densor1 to propagate concat through a small fully-connected neural network to compute the \"intermediate energies\" variable e\n",
        "    e = densor1(concat) \n",
        "    # We use densor2 to propagate e through a small fully-connected neural network to compute the \"energies\" variable energies\n",
        "    energies = densor2(e)\n",
        "    # We use \"activator\" on \"energies\" to compute the attention weights \"alphas\"\n",
        "    alphas = activator(energies)\n",
        "    # We use dotor together with \"alphas\" and \"a\" to compute the context vector to be given to the next (post-attention) LSTM-cell (≈ 1 line)\n",
        "    context = dotor([alphas, a])\n",
        "    \n",
        "    return context"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50OeBdO1n2GD"
      },
      "source": [
        "def model(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    Tx -- length of the input sequence\n",
        "    Ty -- length of the output sequence\n",
        "    n_a -- hidden state size of the Bi-LSTM\n",
        "    n_s -- hidden state size of the post-attention LSTM\n",
        "    human_vocab_size -- size of the python dictionary \"human_vocab\"\n",
        "    machine_vocab_size -- size of the python dictionary \"machine_vocab\"\n",
        "\n",
        "    Returns:\n",
        "    model -- Keras model instance\n",
        "    \"\"\"\n",
        "\n",
        "    X = Input(shape=(Tx, human_vocab_size))\n",
        "    s0 = Input(shape=(n_s,), name='s0')\n",
        "    c0 = Input(shape=(n_s,), name='c0')\n",
        "    s = s0\n",
        "    c = c0\n",
        "    \n",
        "    # Initializing empty list of outputs\n",
        "    outputs = []\n",
        "    \n",
        "    # Defining pre-attention Bi-LSTM\n",
        "    a = Bidirectional(LSTM(n_a, return_sequences=True))(X)\n",
        "    \n",
        "    # Iterating for Ty steps\n",
        "    for t in range(Ty):\n",
        "    \n",
        "        # Performing one step of the attention mechanism to get back the context vector at step t\n",
        "        context = one_step_attention(a, s)\n",
        "        \n",
        "        # Applying the post-attention LSTM cell to the \"context\" vector\n",
        "        s, _, c = post_activation_LSTM_cell(context, initial_state=[s, c])\n",
        "        \n",
        "        # Applying Dense layer to the hidden state output of the post-attention LSTM\n",
        "        out = output_layer(s)\n",
        "        \n",
        "        # Appending \"out\" to the \"outputs\" list\n",
        "        outputs.append(out)\n",
        "    \n",
        "    # Creating model instance taking three inputs and returning the list of outputs\n",
        "    model = Model(inputs=[X, s0, c0], outputs=outputs)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-78PGymn4tY"
      },
      "source": [
        "def gen_date_recog_model():\n",
        "  # Creating date recognition model\n",
        "  date_model = model(Tx, Ty, n_a, n_s, len(human_vocab), len(machine_vocab))\n",
        "  # Loading pretrained weights for faster execution\n",
        "  date_model.load_weights('nmt_weights.h5')\n",
        "  return date_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwTGMgVjeGJI"
      },
      "source": [
        "# Creating the nmt (Neural Machine Translation) model\n",
        "model = gen_date_recog_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTFYCs7peHeD"
      },
      "source": [
        "# This function takes a string of any date format as input and converts it into YYYY-MM-DD format\n",
        "def date_recognizer(date):\n",
        "  source = string_to_int(date, Tx, human_vocab)\n",
        "  source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source))).swapaxes(0,1)\n",
        "  prediction = model.predict([source, s0, c0])\n",
        "  prediction = np.argmax(prediction, axis = -1)\n",
        "  output = [inv_machine_vocab[int(i)] for i in prediction]\n",
        "  return ''.join(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNGJ6kCXtP7i"
      },
      "source": [
        "This function takes the dates dataset we got from the previous step and sends it to the date recognition function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQ0tjarfN13x"
      },
      "source": [
        "# This function is recognising 12,000 dates for the given dataset (Might take around a minute to run)\n",
        "def recognise_dates(date_df, dates):\n",
        "  for date in dates:\n",
        "    date_df[date] = date_df[date].apply(date_recognizer)\n",
        "  print(\"After recognising dates\")\n",
        "  print(date_df.head())\n",
        "  return date_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wD_nL6IpPdBv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "038aa465-d453-42b4-d3e9-f98998c60f5c"
      },
      "source": [
        "date_diff = recognise_dates(date_dff, dates_l)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After recognising dates\n",
            "       Date_1      Date_2      Date_3\n",
            "0  2003-06-26  2008-08-31  1998-06-21\n",
            "1  1994-09-18  1999-11-04  2007-06-05\n",
            "2  1983-01-04  1988-11-15  1984-11-19\n",
            "3  1991-02-13  1983-03-24  2000-07-15\n",
            "4  2003-08-21  1985-08-10  2014-12-03\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYJLzsXfTV45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "cbb001eb-3b5a-4107-fc01-d5344010dffb"
      },
      "source": [
        "# This is the dates dataset after the dates were recognised\n",
        "date_diff.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date_1</th>\n",
              "      <th>Date_2</th>\n",
              "      <th>Date_3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2003-06-26</td>\n",
              "      <td>2008-08-31</td>\n",
              "      <td>1998-06-21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1994-09-18</td>\n",
              "      <td>1999-11-04</td>\n",
              "      <td>2007-06-05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1983-01-04</td>\n",
              "      <td>1988-11-15</td>\n",
              "      <td>1984-11-19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1991-02-13</td>\n",
              "      <td>1983-03-24</td>\n",
              "      <td>2000-07-15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2003-08-21</td>\n",
              "      <td>1985-08-10</td>\n",
              "      <td>2014-12-03</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Date_1      Date_2      Date_3\n",
              "0  2003-06-26  2008-08-31  1998-06-21\n",
              "1  1994-09-18  1999-11-04  2007-06-05\n",
              "2  1983-01-04  1988-11-15  1984-11-19\n",
              "3  1991-02-13  1983-03-24  2000-07-15\n",
              "4  2003-08-21  1985-08-10  2014-12-03"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9zD2xXI4SAB"
      },
      "source": [
        "from datetime import date as date_converter\n",
        "def parse_dates(val):\n",
        "  try:\n",
        "    y,m,d = val.split('-')\n",
        "    return date_converter(int(y),int(m),int(d))\n",
        "  except:\n",
        "    return \"NAN\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWEkNvHkQLZG"
      },
      "source": [
        "# This function generates the pairwise difference as desired in the question\n",
        "def generate_difference(date_df, dates):\n",
        "  if len(dates) < 2:\n",
        "    print(\"Pair-wise difference not possible\")\n",
        "  date_pairs = list(itertools.combinations(dates, 2))\n",
        "  for date_col in dates:\n",
        "    date_df[date_col] = date_df[date_col].apply(parse_dates)\n",
        "  for date_col in dates:\n",
        "    idx = date_df[date_df[date_col]==\"NAN\"].index\n",
        "    date_df.drop(idx, inplace=True)\n",
        "  for pair in date_pairs:\n",
        "    date_df[f\"{pair[0]} - {pair[1]}\"] = date_df[pair[0]] - date_df[pair[1]]\n",
        "  return date_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIfccpRvQ9_F"
      },
      "source": [
        "date_pair_diff = generate_difference(date_diff, dates_l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvE61YEhUVkk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "aa84496e-c3f2-44f1-9861-16ec7ead3942"
      },
      "source": [
        "# Run this cell to get the desired output\n",
        "date_pair_diff.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date_1</th>\n",
              "      <th>Date_2</th>\n",
              "      <th>Date_3</th>\n",
              "      <th>Date_1 - Date_2</th>\n",
              "      <th>Date_1 - Date_3</th>\n",
              "      <th>Date_2 - Date_3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2003-06-26</td>\n",
              "      <td>2008-08-31</td>\n",
              "      <td>1998-06-21</td>\n",
              "      <td>-1893 days</td>\n",
              "      <td>1831 days</td>\n",
              "      <td>3724 days</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1994-09-18</td>\n",
              "      <td>1999-11-04</td>\n",
              "      <td>2007-06-05</td>\n",
              "      <td>-1873 days</td>\n",
              "      <td>-4643 days</td>\n",
              "      <td>-2770 days</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1983-01-04</td>\n",
              "      <td>1988-11-15</td>\n",
              "      <td>1984-11-19</td>\n",
              "      <td>-2142 days</td>\n",
              "      <td>-685 days</td>\n",
              "      <td>1457 days</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1991-02-13</td>\n",
              "      <td>1983-03-24</td>\n",
              "      <td>2000-07-15</td>\n",
              "      <td>2883 days</td>\n",
              "      <td>-3440 days</td>\n",
              "      <td>-6323 days</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2003-08-21</td>\n",
              "      <td>1985-08-10</td>\n",
              "      <td>2014-12-03</td>\n",
              "      <td>6585 days</td>\n",
              "      <td>-4122 days</td>\n",
              "      <td>-10707 days</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Date_1      Date_2  ... Date_1 - Date_3 Date_2 - Date_3\n",
              "0  2003-06-26  2008-08-31  ...       1831 days       3724 days\n",
              "1  1994-09-18  1999-11-04  ...      -4643 days      -2770 days\n",
              "2  1983-01-04  1988-11-15  ...       -685 days       1457 days\n",
              "3  1991-02-13  1983-03-24  ...      -3440 days      -6323 days\n",
              "4  2003-08-21  1985-08-10  ...      -4122 days     -10707 days\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIMv-leFuob4"
      },
      "source": [
        "## I've kept the implementation of the models used here in a separate notebook to prevent cluttering. You can check them through this link if required:\n",
        "\n",
        "* [Date Classifier](https://colab.research.google.com/drive/1pmFRFWzBDYwzuFZv834lzXG9wTROs5ZQ?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mS8s1qxNv0dz"
      },
      "source": [
        "Thank You"
      ]
    }
  ]
}